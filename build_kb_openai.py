"""
æ„å»ºçŸ¥è¯†åº“ï¼ˆä½¿ç”¨ OpenAI Embeddingsï¼‰
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

def load_markdown_files(directory):
    """åŠ è½½ Markdown æ–‡ä»¶"""
    docs = []
    dir_path = Path(directory)

    for md_file in dir_path.glob("*.md"):
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()

        doc = Document(
            page_content=content,
            metadata={"source": str(md_file)}
        )
        docs.append(doc)

    return docs

print("=" * 60)
print("æ„å»ºçŸ¥è¯†åº“ï¼ˆä½¿ç”¨ OpenAI Embeddingsï¼‰")
print("=" * 60)

# æ–‡æ¡£ç›®å½•
docs_dir = "data/documents/apifox"

# åŠ è½½æ–‡æ¡£
print(f"\n1. åŠ è½½æ–‡æ¡£ä»: {docs_dir}")
documents = load_markdown_files(docs_dir)
print(f"   åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ†å‰²æ–‡æ¡£
print("\n2. åˆ†å‰²æ–‡æ¡£")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
)

splits = text_splitter.split_documents(documents)
print(f"   åˆ†å‰²ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

# åˆå§‹åŒ– OpenAI Embeddings
print("\n3. åˆå§‹åŒ– OpenAI Embeddings")
print("   ä¸éœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨ API")

embeddings = OpenAIEmbeddings(
    openai_api_key=os.getenv("OPENAI_API_KEY")
)
print("   OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸ!")

# åˆ›å»ºå‘é‡æ•°æ®åº“
print("\n4. åˆ›å»ºå‘é‡æ•°æ®åº“")
print("   ä¿å­˜åˆ°: data/vectordb")

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="data/vectordb"
)

print(f"   å‘é‡æ•°æ®åº“å·²åˆ›å»º!")

# ç»Ÿè®¡ä¿¡æ¯
collection = vectorstore.get()
print(f"\n5. ç»Ÿè®¡ä¿¡æ¯")
print(f"   æ€»å‘é‡æ•°: {len(collection.get('ids', []))}")

# æµ‹è¯•æ£€ç´¢
print("\n6. æµ‹è¯•æ£€ç´¢åŠŸèƒ½")
test_queries = [
    "æ¥å£æµ‹è¯•å¤±è´¥æ€ä¹ˆåŠ",
    "å¦‚ä½•é…ç½®ç¯å¢ƒå˜é‡",
    "è¿”å› 500 é”™è¯¯"
]

for query in test_queries:
    print(f"\næŸ¥è¯¢: {query}")
    results = vectorstore.similarity_search_with_score(query, k=2)

    for i, (doc, score) in enumerate(results, 1):
        content = doc.page_content[:80].replace('\n', ' ')
        print(f"  {i}. (ç›¸ä¼¼åº¦: {score:.4f}) {content}...")

print("\n" + "=" * 60)
print("çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
print("=" * 60)
print("\nğŸ“ ä¿å­˜ä½ç½®: data/vectordb/")
print("\nä¸‹ä¸€æ­¥:")
print("1. æµ‹è¯•é—®é¢˜åˆ†ç±»ï¼ˆä½¿ç”¨ LLM + çŸ¥è¯†åº“ï¼‰")
print("2. æµ‹è¯•å®Œæ•´æµç¨‹")
print("3. é…ç½®é£ä¹¦åº”ç”¨")

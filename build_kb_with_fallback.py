"""
æ„å»ºçŸ¥è¯†åº“ï¼ˆä½¿ç”¨é•œåƒå’Œé™çº§æ–¹æ¡ˆï¼‰
"""

import os
from pathlib import Path

# è®¾ç½® HuggingFace é•œåƒï¼ˆè§£å†³å›½å†…ç½‘ç»œé—®é¢˜ï¼‰
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

def load_markdown_files(directory):
    """åŠ è½½ Markdown æ–‡ä»¶"""
    docs = []
    dir_path = Path(directory)

    for md_file in dir_path.glob("*.md"):
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read()

        doc = Document(
            page_content=content,
            metadata={"source": str(md_file)}
        )
        docs.append(doc)

    return docs

print("=" * 60)
print("æ„å»ºçŸ¥è¯†åº“ï¼ˆä½¿ç”¨é•œåƒåŠ é€Ÿï¼‰")
print("=" * 60)

# æ–‡æ¡£ç›®å½•
docs_dir = "data/documents/apifox"

# åŠ è½½æ–‡æ¡£
print(f"\n1. åŠ è½½æ–‡æ¡£ä»: {docs_dir}")
documents = load_markdown_files(docs_dir)
print(f"   åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# åˆ†å‰²æ–‡æ¡£
print("\n2. åˆ†å‰²æ–‡æ¡£")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
)

splits = text_splitter.split_documents(documents)
print(f"   åˆ†å‰²ä¸º {len(splits)} ä¸ªç‰‡æ®µ")

# å°è¯•æ–¹æ¡ˆ 1: ä½¿ç”¨é•œåƒä¸‹è½½æ¨¡å‹
print("\n3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹")
print("   æ–¹æ¡ˆ 1: ä½¿ç”¨ hf-mirror é•œåƒ")
print("   æ¨¡å‹: shibing624/text2vec-base-chinese")

try:
    from langchain_community.embeddings import HuggingFaceEmbeddings

    embeddings = HuggingFaceEmbeddings(
        model_name="shibing624/text2vec-base-chinese",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    print("   âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")

except Exception as e:
    print(f"   âœ— é•œåƒä¸‹è½½å¤±è´¥: {e}")

    # å°è¯•æ–¹æ¡ˆ 2: ä½¿ç”¨ OpenAI Embeddings
    print("\n   æ–¹æ¡ˆ 2: ä½¿ç”¨ OpenAI Embeddings")

    try:
        from dotenv import load_dotenv
        load_dotenv()

        from langchain_openai import OpenAIEmbeddings

        embeddings = OpenAIEmbeddings(
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        print("   âœ“ ä½¿ç”¨ OpenAI Embeddings")

    except Exception as e2:
        print(f"   âœ— OpenAI ä¹Ÿå¤±è´¥äº†: {e2}")

        # æ–¹æ¡ˆ 3: ä½¿ç”¨ç®€å•çš„è¯å‘é‡ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        print("\n   æ–¹æ¡ˆ 3: ä½¿ç”¨ Chroma é»˜è®¤åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆï¼‰")

        from chromadb.utils import embedding_functions
        embeddings = embedding_functions.DefaultEmbeddingFunction()
        print("   âœ“ ä½¿ç”¨é»˜è®¤åµŒå…¥å‡½æ•°")

# åˆ›å»ºå‘é‡æ•°æ®åº“
print("\n4. åˆ›å»ºå‘é‡æ•°æ®åº“")
print("   ä¿å­˜åˆ°: data/vectordb")

try:
    from langchain_community.vectorstores import Chroma

    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory="data/vectordb"
    )

    print(f"   âœ“ å‘é‡æ•°æ®åº“å·²åˆ›å»º!")

    # ç»Ÿè®¡ä¿¡æ¯
    collection = vectorstore.get()
    print(f"\n5. ç»Ÿè®¡ä¿¡æ¯")
    print(f"   æ€»æ–‡æ¡£æ•°: {len(collection.get('ids', []))}")

    # æµ‹è¯•æ£€ç´¢
    print("\n6. æµ‹è¯•æ£€ç´¢")
    test_queries = [
        "æ¥å£æµ‹è¯•å¤±è´¥æ€ä¹ˆåŠ",
        "å¦‚ä½•é…ç½®ç¯å¢ƒå˜é‡",
        "è¿”å› 500 é”™è¯¯"
    ]

    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        results = vectorstore.similarity_search_with_score(query, k=2)

        for i, (doc, score) in enumerate(results, 1):
            content = doc.page_content[:80].replace('\n', ' ')
            print(f"  {i}. (ç›¸ä¼¼åº¦: {score:.4f}) {content}...")

    print("\n" + "=" * 60)
    print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
    print("=" * 60)
    print("\nğŸ“ ä¿å­˜ä½ç½®: data/vectordb/")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. æµ‹è¯•é—®é¢˜åˆ†ç±»ï¼ˆä½¿ç”¨ LLM + çŸ¥è¯†åº“ï¼‰")
    print("2. æµ‹è¯•å®Œæ•´æµç¨‹")
    print("3. é…ç½®é£ä¹¦åº”ç”¨")

except Exception as e:
    print(f"\nâœ— æ„å»ºå¤±è´¥: {e}")
    print("\nå»ºè®®: ä½¿ç”¨ OpenAI Embeddingsï¼ˆæ–¹æ¡ˆBï¼‰")
